# Deep-Learning-Papers [Incomplete List]
This repository consist of various Deep Learning papers sorted by subareas. Currently this list is incomplete , I will add them overtime. I will also update this repository as I come across more papers that are interesting. Please feel free to make a pull request and add to the list.  

## General Deep Learning

* <a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a>
* <a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a>
* <a href="https://arxiv.org/abs/1805.02641">Label Refinery: Improving ImageNet Classification through Label Progression</a>

## Industrial Machine Learning

* A Github Repository <a href="https://github.com/eugeneyan/applied-ml ">Repository</a> that did a great job curating resources

## Generalization 

* <a href="https://arxiv.org/pdf/2005.14165.pdf">Do CIFAR-10 Classifiers Generalize to CIFAR-10?</a>

## Explainability

* <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3375624">Explainable Machine Learning in Deployment</a>

## Natural Language Processing 

### GPT

* GPT-1: <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving Language Understanding
by Generative Pre-Training</a>
* GPT-2: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>
* GPT-3: <a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a>

## Computer Vision 

### Imagenet Architectures 

* Alexnet: <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet Classification with Deep Convolutional Neural Networks</a>
* VGG: <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>
* Resnet: <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>
* Densenet: <a href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks</a>
* Mobilenet: <a href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a>
* Squeezenet: <a href="https://arxiv.org/abs/1602.07360">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size</a>
  
 ### Scene Understanding 
 
 * <a href="https://rgbd.cs.princeton.edu/paper.pdf">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</a>

### Weakly Supervised Object Detection 

* <a href="https://arxiv.org/abs/1704.00138">Multiple Instance Detection Network with Online Instance Classifier Refinement</a>
* <a href="https://arxiv.org/abs/1904.00551">Weakly Supervised Object Detection with Segmentation Collaboration</a>
* <a href="https://arxiv.org/abs/1511.02853">Weakly Supervised Deep Detection Networks</a> 

### Understanding and Visualizing

* <a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>
* <a href="https://arxiv.org/abs/2010.15327">Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth</a>

## Faster Training
* <a href="https://arxiv.org/pdf/1708.03888.pdf">LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS</a> : Introduces adaptive layerwise learning rates. Different learning rates for different layers. 
* <a href="https://arxiv.org/abs/1910.00762">Accelerating Deep Learning by Focusing on the Biggest Losers</a>

## Meta Learning

* MAML: <a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>
* Reptile: <a href="https://openai.com/blog/reptile/">On First-Order Meta-Learning Algorithms</a>

## Multitask Learning

* <a href="https://daiwk.github.io/assets/youtube-multitask.pdf">Recommending What Video to Watch Next: A Multitask Ranking System</a>
* <a href="https://arxiv.org/pdf/1707.08114.pdf">A Survey on Multi-Task Learning</a>
* <a href="https://arxiv.org/pdf/1705.07115.pdf">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</a>

## Zero shot Learning

* <a href="https://arxiv.org/pdf/1707.00600.pdf">Zero-Shot Learning -- The Good, the Bad and the Ugly</a>
* <a href="https://arxiv.org/pdf/1803.11320.pdf">Transductive Unbiased Embedding for Zero-Shot Learning</a>
* <a href="https://arxiv.org/abs/1704.08345">Semantic Autoencoder for Zero-Shot Learning</a>

## Bayesian Deep Learning

* <a href="https://arxiv.org/abs/1506.02158">Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</a>
* <a href="https://arxiv.org/abs/1703.04977">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</a>

## Privacy Preserving Learning

### Federated Learning: Data Security 

* <a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>
* <a href="https://arxiv.org/abs/1610.05492">Federated Learning: Strategies for Improving Communication Efficiency</a>
* <a href="https://research.google/pubs/pub47246/">Practical Secure Aggregation for Privacy-Preserving Machine Learning</a>
* <a href="https://arxiv.org/abs/1909.12641">Active Federated Learning</a>
* <a href="https://arxiv.org/abs/1712.01887">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a>
* <a href="https://arxiv.org/abs/1806.00582">Federated Learning with Non-IID Data</a>
* <a href="https://arxiv.org/abs/1912.04977">Advances and Open Problems in Federated Learning</a>

### Model Security (Inference)

* <a href="https://arxiv.org/abs/1806.10313">DeepObfuscation: Securing the Structure of Convolutional Neural Networks via Knowledge Distillation
</a>

### Privacy Attacks & Defenses 
* <a href="https://bair.berkeley.edu/blog/2019/08/13/memorization/">Evaluating and Testing Unintended Memorization in Neural Networks</a>
* <a href="https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html">Privacy Considerations in Large Language Models</a>

## Neural Network Compression

* <a href="https://arxiv.org/abs/1510.00149">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a>
* <a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>
* <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a>
* <a href="https://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a>

